{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Preetam314/pytorch_CV/blob/main/A3_nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SH6VHVv1XwvR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('reviews.csv')\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If sentiment is text (positive/negative), convert to binary\n",
        "if df['sentiment'].dtype == 'object':\n",
        "    df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})"
      ],
      "metadata": {
        "id": "x4rOtTKtWpe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Cleaning Function\n",
        "def clean_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    # Remove special characters and numbers (keep letters and spaces)\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    # Remove extra whitespace\n",
        "    text = ' '.join(text.split())\n",
        "    return text"
      ],
      "metadata": {
        "id": "EK5pY1ZXWrIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply cleaning\n",
        "df['cleaned_review'] = df['review'].apply(clean_text)\n",
        "print(\"\\nSample cleaned reviews:\")\n",
        "print(df['cleaned_review'].head())"
      ],
      "metadata": {
        "id": "vNbVox99Wsor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "def tokenize(text):\n",
        "    return text.split()\n",
        "\n",
        "df['tokens'] = df['cleaned_review'].apply(tokenize)\n",
        "\n",
        "# Build Vocabulary\n",
        "all_words = []\n",
        "for tokens in df['tokens']:\n",
        "    all_words.extend(tokens)\n",
        "\n",
        "word_counts = Counter(all_words)\n",
        "print(f\"\\nTotal unique words: {len(word_counts)}\")\n",
        "\n",
        "vocab_size = 10000  # Limit vocabulary size\n",
        "most_common = word_counts.most_common(vocab_size - 1)\n",
        "word2idx = {word: idx + 1 for idx, (word, count) in enumerate(most_common)}\n",
        "word2idx['<PAD>'] = 0\n",
        "word2idx['<UNK>'] = len(word2idx)  # Unknown words\n",
        "\n",
        "print(f\"Vocabulary size: {len(word2idx)}\")\n"
      ],
      "metadata": {
        "id": "00PeRRS3WuL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert tokens to indices\n",
        "def tokens_to_indices(tokens, word2idx):\n",
        "    \"\"\"Convert tokens to integer indices\"\"\"\n",
        "    return [word2idx.get(word, word2idx['<UNK>']) for word in tokens]\n",
        "\n",
        "df['indices'] = df['tokens'].apply(lambda x: tokens_to_indices(x, word2idx))"
      ],
      "metadata": {
        "id": "7_wy9VbIWvpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Padding and Truncation\n",
        "max_length = 200  # Fixed sequence length\n",
        "\n",
        "def pad_truncate(sequence, max_len):\n",
        "    \"\"\"Pad or truncate sequence to max_len\"\"\"\n",
        "    if len(sequence) > max_len:\n",
        "        return sequence[:max_len]\n",
        "    else:\n",
        "        return sequence + [0] * (max_len - len(sequence))\n",
        "\n",
        "df['padded_indices'] = df['indices'].apply(lambda x: pad_truncate(x, max_length))\n"
      ],
      "metadata": {
        "id": "UywyySJCWxI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare final data\n",
        "X = np.array(df['padded_indices'].tolist())\n",
        "y = np.array(df['sentiment'].values)\n",
        "\n",
        "print(f\"\\nX shape: {X.shape}\")\n",
        "print(f\"y shape: {y.shape}\")\n",
        "\n",
        "# Split dataset: 70% train, 15% validation, 15% test\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
        "\n",
        "print(f\"\\nTrain set: {X_train.shape}, {y_train.shape}\")\n",
        "print(f\"Validation set: {X_val.shape}, {y_val.shape}\")\n",
        "print(f\"Test set: {X_test.shape}, {y_test.shape}\")\n",
        "\n",
        "# Save preprocessed data\n",
        "np.save('X_train.npy', X_train)\n",
        "np.save('X_val.npy', X_val)\n",
        "np.save('X_test.npy', X_test)\n",
        "np.save('y_train.npy', y_train)\n",
        "np.save('y_val.npy', y_val)\n",
        "np.save('y_test.npy', y_test)\n"
      ],
      "metadata": {
        "id": "PJ9-1S9aWydo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save vocabulary\n",
        "import pickle\n",
        "with open('word2idx.pkl', 'wb') as f:\n",
        "    pickle.dump(word2idx, f)\n",
        "\n",
        "print(\"\\nPreprocessing complete! Files saved.\")"
      ],
      "metadata": {
        "id": "crCiF5dsW0Am"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n"
      ],
      "metadata": {
        "id": "jacsD9uGW140"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.LongTensor(X_train)\n",
        "y_train_tensor = torch.FloatTensor(y_train)\n",
        "X_val_tensor = torch.LongTensor(X_val)\n",
        "y_val_tensor = torch.FloatTensor(y_val)\n",
        "X_test_tensor = torch.LongTensor(X_test)\n",
        "y_test_tensor = torch.FloatTensor(y_test)\n"
      ],
      "metadata": {
        "id": "uk192_SkW3b3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create DataLoaders\n",
        "batch_size = 64\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "K7Z8rAHCW48z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define RNN Model\n",
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers=1, dropout=0.3):\n",
        "        super(RNNClassifier, self).__init__()\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "\n",
        "        # RNN layer\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers=n_layers,\n",
        "                          batch_first=True, dropout=dropout if n_layers > 1 else 0)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Fully connected output layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_length)\n",
        "\n",
        "        # Embedding\n",
        "        embedded = self.embedding(x)  # (batch_size, seq_length, embedding_dim)\n",
        "\n",
        "        # RNN\n",
        "        output, hidden = self.rnn(embedded)  # output: (batch_size, seq_length, hidden_dim)\n",
        "\n",
        "        # Take the last hidden state\n",
        "        last_hidden = output[:, -1, :]  # (batch_size, hidden_dim)\n",
        "\n",
        "        # Dropout\n",
        "        dropped = self.dropout(last_hidden)\n",
        "\n",
        "        # Fully connected\n",
        "        out = self.fc(dropped)  # (batch_size, output_dim)\n",
        "\n",
        "        return out.squeeze()\n"
      ],
      "metadata": {
        "id": "9QVrtsEqW6Kd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #Hyperparameters\n",
        "vocab_size = 10002  # Adjust based on your vocabulary size\n",
        "embedding_dim = 200\n",
        "hidden_dim = 128\n",
        "output_dim = 1\n",
        "n_layers = 2\n",
        "dropout = 0.3\n",
        "learning_rate = 0.001\n",
        "\n"
      ],
      "metadata": {
        "id": "6LU86Qk4W7hI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = RNNClassifier(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout)\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "print(f\"\\nRNN Model Architecture:\")\n",
        "print(model)\n",
        "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters())}\")"
      ],
      "metadata": {
        "id": "T6Kia9FwW8w7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_x, batch_y in tqdm(train_loader, desc=\"Training\"):\n",
        "        batch_x = batch_x.to(device)\n",
        "        batch_y = batch_y.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_x)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping (prevents exploding gradients)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate metrics\n",
        "        total_loss += loss.item()\n",
        "        predictions = torch.sigmoid(outputs) > 0.5\n",
        "        correct += (predictions == batch_y).sum().item()\n",
        "        total += batch_y.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = correct / total\n",
        "    return avg_loss, accuracy"
      ],
      "metadata": {
        "id": "qG7W3_m1XABe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in val_loader:\n",
        "            batch_x = batch_x.to(device)\n",
        "            batch_y = batch_y.to(device)\n",
        "\n",
        "            outputs = model(batch_x)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            predictions = torch.sigmoid(outputs) > 0.5\n",
        "            correct += (predictions == batch_y).sum().item()\n",
        "            total += batch_y.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(val_loader)\n",
        "    accuracy = correct / total\n",
        "    return avg_loss, accuracy"
      ],
      "metadata": {
        "id": "EBs9-MdWXBcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs=10):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accuracies = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Train\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "\n",
        "        # Validate\n",
        "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_acc)\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), 'best_rnn_model.pt')\n",
        "            print(\"✓ Best model saved!\")\n",
        "\n",
        "    return {\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'train_accuracies': train_accuracies,\n",
        "        'val_accuracies': val_accuracies\n",
        "    }\n",
        "    # Train the RNN model\n",
        "print(\"Training RNN Model...\")\n",
        "rnn_history = train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs=10)\n",
        "\n",
        "# Save training history\n",
        "import pickle\n",
        "with open('rnn_history.pkl', 'wb') as f:\n",
        "    pickle.dump(rnn_history, f)\n",
        "\n"
      ],
      "metadata": {
        "id": "1MU4xbOrXCwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    \"\"\"Evaluate model on test set\"\"\"\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    all_probabilities = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in test_loader:\n",
        "            batch_x = batch_x.to(device)\n",
        "            batch_y = batch_y.to(device)\n",
        "\n",
        "            outputs = model(batch_x)\n",
        "            probabilities = torch.sigmoid(outputs)\n",
        "            predictions = probabilities > 0.5\n",
        "\n",
        "            all_predictions.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(batch_y.cpu().numpy())\n",
        "            all_probabilities.extend(probabilities.cpu().numpy())\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    predictions = np.array(all_predictions).astype(int)\n",
        "    labels = np.array(all_labels).astype(int)\n",
        "    probabilities = np.array(all_probabilities)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    precision = precision_score(labels, predictions)\n",
        "    recall = recall_score(labels, predictions)\n",
        "    f1 = f1_score(labels, predictions)\n",
        "\n",
        "    metrics = {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'predictions': predictions,\n",
        "        'labels': labels,\n",
        "        'probabilities': probabilities\n",
        "    }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def print_metrics(model_name, metrics):\n",
        "    \"\"\"Print evaluation metrics\"\"\"\n",
        "    print(f\"\\n{model_name} Test Set Metrics:\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Accuracy:  {metrics['accuracy']:.4f}\")\n",
        "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
        "    print(f\"Recall:    {metrics['recall']:.4f}\")\n",
        "    print(f\"F1-Score:  {metrics['f1_score']:.4f}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "def plot_confusion_matrix(labels, predictions, model_name):\n",
        "    \"\"\"Plot confusion matrix\"\"\"\n",
        "    cm = confusion_matrix(labels, predictions)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Negative', 'Positive'],\n",
        "                yticklabels=['Negative', 'Positive'])\n",
        "    plt.title(f'Confusion Matrix - {model_name}')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{model_name.lower()}_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "otRUFRGcXEyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate RNN\n",
        "print(\"\\nEvaluating RNN Model...\")\n",
        "model.load_state_dict(torch.load('best_rnn_model.pt'))\n",
        "rnn_metrics = evaluate_model(model, test_loader, device)\n",
        "print_metrics(\"RNN\", rnn_metrics)\n",
        "plot_confusion_matrix(rnn_metrics['labels'], rnn_metrics['predictions'], \"RNN\")"
      ],
      "metadata": {
        "id": "Ewzf9-50XGY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "\n",
        "# Load training histories\n",
        "with open('rnn_history.pkl', 'rb') as f:\n",
        "    rnn_history = pickle.load(f)\n",
        "\n",
        "# Plot Loss vs Epochs (All models)\n",
        "def plot_all_losses():\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "    # Training Loss\n",
        "    axes[0].plot(rnn_history['train_losses'], label='RNN', marker='o', linewidth=2)\n",
        "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
        "    axes[0].set_ylabel('Training Loss', fontsize=12)\n",
        "    axes[0].set_title('Training Loss vs Epochs', fontsize=14, fontweight='bold')\n",
        "    axes[0].legend(fontsize=11)\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Validation Loss\n",
        "    axes[1].plot(rnn_history['val_losses'], label='RNN', marker='o', linewidth=2)\n",
        "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
        "    axes[1].set_ylabel('Validation Loss', fontsize=12)\n",
        "    axes[1].set_title('Validation Loss vs Epochs', fontsize=14, fontweight='bold')\n",
        "    axes[1].legend(fontsize=11)\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('loss_comparison_all_models.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# Plot Accuracy vs Epochs\n",
        "def plot_all_accuracies():\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "    # Training Accuracy\n",
        "    axes[0].plot(rnn_history['train_accuracies'], label='RNN', marker='o', linewidth=2)\n",
        "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
        "    axes[0].set_ylabel('Training Accuracy', fontsize=12)\n",
        "    axes[0].set_title('Training Accuracy vs Epochs', fontsize=14, fontweight='bold')\n",
        "    axes[0].legend(fontsize=11)\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Validation Accuracy\n",
        "    axes[1].plot(rnn_history['val_accuracies'], label='RNN', marker='o', linewidth=2)\n",
        "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
        "    axes[1].set_ylabel('Validation Accuracy', fontsize=12)\n",
        "    axes[1].set_title('Validation Accuracy vs Epochs', fontsize=14, fontweight='bold')\n",
        "    axes[1].legend(fontsize=11)\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('accuracy_comparison_all_models.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# Individual model plots\n",
        "def plot_single_model(history, model_name):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Loss\n",
        "    axes[0].plot(history['train_losses'], label='Train Loss', marker='o', linewidth=2)\n",
        "    axes[0].plot(history['val_losses'], label='Val Loss', marker='s', linewidth=2)\n",
        "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
        "    axes[0].set_ylabel('Loss', fontsize=12)\n",
        "    axes[0].set_title(f'{model_name} - Loss vs Epochs', fontsize=14, fontweight='bold')\n",
        "    axes[0].legend(fontsize=11)\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Accuracy\n",
        "    axes[1].plot(history['train_accuracies'], label='Train Accuracy', marker='o', linewidth=2)\n",
        "    axes[1].plot(history['val_accuracies'], label='Val Accuracy', marker='s', linewidth=2)\n",
        "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
        "    axes[1].set_ylabel('Accuracy', fontsize=12)\n",
        "    axes[1].set_title(f'{model_name} - Accuracy vs Epochs', fontsize=14, fontweight='bold')\n",
        "    axes[1].legend(fontsize=11)\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{model_name.lower()}_training_history.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# Generate all plots\n",
        "print(\"Generating comparison plots...\")\n",
        "plot_all_losses()\n",
        "plot_all_accuracies()\n",
        "\n",
        "print(\"\\nGenerating individual model plots...\")\n",
        "plot_single_model(rnn_history, \"RNN\")\n",
        "\n",
        "print(\"All plots saved successfully!\")"
      ],
      "metadata": {
        "id": "lXjcrb2ZXH23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create comparison table (RNN only)\n",
        "def create_comparison_table(rnn_metrics):\n",
        "    \"\"\"Create a comparison table for RNN model only\"\"\"\n",
        "\n",
        "    comparison_data = {\n",
        "        'Model': ['RNN'],\n",
        "        'Accuracy': [rnn_metrics['accuracy']],\n",
        "        'Precision': [rnn_metrics['precision']],\n",
        "        'Recall': [rnn_metrics['recall']],\n",
        "        'F1-Score': [rnn_metrics['f1_score']]\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame(comparison_data)\n",
        "\n",
        "    # Format to 4 decimal places\n",
        "    for col in ['Accuracy', 'Precision', 'Recall', 'F1-Score']:\n",
        "        df[col] = df[col].apply(lambda x: f\"{x:.4f}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Create and display RNN performance table\n",
        "comparison_df = create_comparison_table(rnn_metrics)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"RNN MODEL PERFORMANCE - TEST SET\")\n",
        "print(\"=\"*70)\n",
        "print(comparison_df.to_string(index=False))\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Save to CSV\n",
        "comparison_df.to_csv('rnn_model_performance.csv', index=False)\n",
        "print(\"\\n✓ RNN performance table saved to 'rnn_model_performance.csv'\")\n",
        "\n",
        "# Visualize RNN metrics\n",
        "def plot_rnn_metrics(comparison_df):\n",
        "    \"\"\"Create a bar chart for RNN model performance\"\"\"\n",
        "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "    values = [float(comparison_df.loc[0, metric]) for metric in metrics]\n",
        "\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    bars = plt.bar(metrics, values, color='skyblue', width=0.5)\n",
        "\n",
        "    plt.xlabel('Metrics', fontsize=12, fontweight='bold')\n",
        "    plt.ylabel('Score', fontsize=12, fontweight='bold')\n",
        "    plt.title('RNN Model Performance on Test Set', fontsize=14, fontweight='bold')\n",
        "    plt.ylim([0, 1.0])\n",
        "    plt.grid(True, axis='y', alpha=0.3)\n",
        "\n",
        "    # Add value labels\n",
        "    for bar in bars:\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
        "                 f\"{bar.get_height():.3f}\", ha='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('rnn_metrics_bar_chart.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "plot_rnn_metrics(comparison_df)\n"
      ],
      "metadata": {
        "id": "sgccWEdaXJoG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}